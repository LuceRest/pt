{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "\n",
      "\n",
      "Virus Corona atau severe acute respiratory syndrome coronavirus 2\n",
      "\n",
      "(SARS-CoV-2) adalah virus yang menyerang sistem pernapasan. \n",
      "\n",
      "Penyakit akibat infeksi virus ini disebut COVID-19. Virus Corona bisa \n",
      "\n",
      "menyebabkan gangguan ringan pada sistem pernapasan, infeksi paru-paru yang berat, \n",
      "\n",
      "hingga kematian.\n",
      "\n",
      "\n",
      "\n",
      "After removing stop words from the said text:\n",
      "['Virus', 'Corona', 'atau', 'severe', 'acute', 'respiratory', 'syndrome', 'coronavirus', '2', '(SARS-CoV-2)', 'adalah', 'virus', 'yang', 'menyerang', 'sistem', 'pernapasan.', 'Penyakit', 'akibat', 'infeksi', 'virus', 'ini', 'disebut', 'COVID-19.', 'Virus', 'Corona', 'bisa', 'menyebabkan', 'gangguan', 'ringan', 'pada', 'sistem', 'pernapasan,', 'infeksi', 'paru-paru', 'yang', 'berat,', 'hingga', 'kematian.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words ('english')\n",
    "\n",
    "text = '''\n",
    "\n",
    "Virus Corona atau severe acute respiratory syndrome coronavirus 2\n",
    "\n",
    "(SARS-CoV-2) adalah virus yang menyerang sistem pernapasan. \n",
    "\n",
    "Penyakit akibat infeksi virus ini disebut COVID-19. Virus Corona bisa \n",
    "\n",
    "menyebabkan gangguan ringan pada sistem pernapasan, infeksi paru-paru yang berat, \n",
    "\n",
    "hingga kematian.\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\nOriginal string:\")\n",
    "\n",
    "print(text )\n",
    "\n",
    "clean_word_list = [ word for word in text.split () if word not in stoplist]\n",
    "\n",
    "print(\"\\nAfter removing stop words from the said text:\")\n",
    "\n",
    "print(clean_word_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original string:\n",
      "Reset your password if you just can't remember your old one.\n",
      "\n",
      "Split all punctuation into separate tokens:\n",
      "['Reset', 'your', 'password', 'if', 'you', 'just', 'can', \"'\", 't', 'remember', 'your', 'old', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = \"Reset your password if you just can't remember your old one.\"\n",
    "\n",
    "print (\"\\nOriginal string:\")\n",
    "\n",
    "print(text)\n",
    "\n",
    "result = WordPunctTokenizer() .tokenize(text )\n",
    "\n",
    "print(\"\\nSplit all punctuation into separate tokens:\")\n",
    "\n",
    "print(result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Makan',\n",
       " 'bersama',\n",
       " 'kawan',\n",
       " 'di',\n",
       " 'Jogja',\n",
       " 'jogja',\n",
       " 'liburan',\n",
       " 'liburanakhirpekan',\n",
       " 'Makan',\n",
       " 'siang',\n",
       " 'Pantaikuta',\n",
       " 'restro',\n",
       " 'smile']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\" Makan bersama kawan di Jogja #jogja #liburan #liburanakhirpekan. Makan siang @Pantaikuta restro smile \"\n",
    "import re\n",
    "\n",
    "text=re.sub(r'[^\\w]', ' ', text )\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer=TweetTokenizer ()\n",
    "\n",
    "tokenizer .tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 random labeled combined names:\n",
      "[('Adriaens', 'female'), ('Enrique', 'male'), ('Bonita', 'female'), ('Clarey', 'female'), ('Garwin', 'male'), ('Lyda', 'female'), ('Margery', 'female'), ('Bobette', 'female'), ('Kaja', 'female'), ('Jenda', 'female'), ('Olimpia', 'female'), ('Jedediah', 'male'), ('Leodora', 'female'), ('Polly', 'female'), ('Sharla', 'female')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names \n",
    "import random \n",
    " \n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    " \n",
    "labeled_male_names = [(str(name), 'male') for name in male_names]\n",
    "labeled_female_names = [(str(name), 'female') for name in female_names]\n",
    "labeled_all_names = labeled_male_names + labeled_female_names\n",
    "random.shuffle(labeled_all_names)\n",
    "print(\"First 15 random labeled combined names:\")\n",
    "print (labeled_all_names[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last\n",
      "week\n",
      ",\n",
      "the\n",
      "University\n",
      "of\n",
      "Cambridge\n",
      "shared\n",
      "its\n",
      "own\n",
      "research\n",
      "that\n",
      "shows\n",
      "if\n",
      "everyone\n",
      "wears\n",
      "a\n",
      "mask\n",
      "outside\n",
      "home\n",
      ",\n",
      "dreaded\n",
      "‘\n",
      "second\n",
      "wave\n",
      "’\n",
      "of\n",
      "the\n",
      "pandemic\n",
      "can\n",
      "be\n",
      "avoided\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\"\n",
    "tokens=nltk.word_tokenize(text )\n",
    "for token in tokens :\n",
    "  print (token )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'all', 'is', 'said', 'and', 'done', 'more', 'is', 'said', 'than', 'done']\n"
     ]
    }
   ],
   "source": [
    "saying = \"After all is said and done more is said than done\"\n",
    "bland = saying.split()\n",
    "print(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flsanoosaho'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(w[1] for w in bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saying = ['After', 'all','is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "lengths = []\n",
    "for w in saying :\n",
    "    lengths.append(len(w))\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long-\\nterm', 'encyclo-\\npedia']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "text = 'some text with long-\\nterm and encyclo-\\npedia'\n",
    "words = re.findall(r'\\w+\\-\\n\\w+', text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long-\\nterm', 'encyclo-\\npedia']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "text = 'some text with long-\\nterm and encyclo-\\npedia'\n",
    "words = re.findall(r'\\w+\\-\\n\\w+', text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some text with long-term and encyclo-pedia\n"
     ]
    }
   ],
   "source": [
    "words = ['some text with long-\\nterm and encyclo-\\npedia']\n",
    "for w in words:\n",
    "    print (re.sub('\\n', '', w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "def insert (trie, key, value):\n",
    "    if key:\n",
    "        first, rest = key[0], key[1:]\n",
    "        if first not in trie:\n",
    "            trie[first] = {}\n",
    "        insert(trie[first], rest, value)\n",
    "    else:\n",
    "        trie['value'] = value\n",
    "        \n",
    "trie = nltk.defaultdict(dict)\n",
    "insert(trie, 'chat', 'cat')\n",
    "insert(trie, 'chien', 'dog')\n",
    "insert(trie, 'chair', 'flesh')\n",
    "trie['c']['h']['a']['t']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whale']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getWords(prop, value):\n",
    "    lexicon = [('fish', 'water animal', 'fish'), ('house', 'building', 'haus'), ('whale', 'water animal', 'wejl')]\n",
    "    if prop == 'meaning':\n",
    "        return [w for (w, m, p) in lexicon if m == value]\n",
    "    if prop == 'pronunciation':\n",
    "        return [w for (w, m, p) in lexicon if p == value]\n",
    "getWords('pronunciation', 'wejl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flesh'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def insert(trie, key, value):\n",
    "    if key:\n",
    "        first, rest = key[0], key[1:]\n",
    "        if first not in trie:\n",
    "            trie[first] = {}\n",
    "        insert(trie[first], rest, value)\n",
    "    else:\n",
    "        trie['value'] = value\n",
    "        \n",
    "trie = nltk.defaultdict(dict)\n",
    "insert(trie, 'chat', 'cat')\n",
    "insert(trie, 'chien', 'dog')\n",
    "insert(trie, 'chair', 'flesh')\n",
    "trie['c']['h']['a']['i']['r']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getWords(prop, value):\n",
    "    lexicon = [('fish', 'water animal', 'fish'), ('house', 'building', 'haus'), ('whale', 'water animal', 'wejl')]\n",
    "    if prop == 'meaning':\n",
    "        return [w for (w, m, p) in lexicon if m == value]\n",
    "    if prop == 'pronunciation':\n",
    "        return [w for (w, m, p) in lexicon if p == value]\n",
    "getWords('pronunciation', 'haus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', '', 'hello'], ['', '', 'hello'], ['', '', 'hello'], ['', '', 'hello']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_table = [[''] * 3] * 4\n",
    "word_table[1][2] = \"hello\"\n",
    "word_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fish', 'whale']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getWords(prop, value):\n",
    "    lexicon = [('fish', 'water animal', 'fish'), ('house', 'building', 'haus'), ('whale', 'water animal', 'wejl')]\n",
    "    if prop == 'meaning':\n",
    "        return [w for (w, m, p) in lexicon if m == value]\n",
    "    if prop == 'pronunciation':\n",
    "        return [w for (w, m, p) in lexicon if p == value]\n",
    "getWords('meaning', 'water animal')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'My', 'phrase', 'unimaginative', 'very']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = ['My', 'very', 'unimaginative', 'phrase', '.']\n",
    "phrase.sort()\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = ['My', 'very', 'unimaginative', 'phrase', '.']\n",
    "phrase[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'very',\n",
       " 'unimaginative',\n",
       " 'phrase',\n",
       " '.',\n",
       " 'My',\n",
       " 'very',\n",
       " 'unimaginative',\n",
       " 'phrase',\n",
       " '.',\n",
       " 'My',\n",
       " 'very',\n",
       " 'unimaginative',\n",
       " 'phrase',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = ['My', 'very', 'unimaginative', 'phrase', '.']\n",
    "phrase + phrase + phrase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
